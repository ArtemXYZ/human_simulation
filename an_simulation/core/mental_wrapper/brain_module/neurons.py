"""Базовая реализация нейрона."""

import numpy as np

# ----------------------------------------------------------------------------------------------------------------------

# Класс для одного нейрона
class Neuron:
    """
    Bias (смещение).
    Термин bias стал стандартом в мире искусственных нейронных сетей, суть его работы — это сдвиг функции активации \
    (в литературе принято называть этот параметр именно bias).
    Bias — это дополнительный параметр в нейронной сети, который добавляется к взвешенной сумме входных сигналов.

    Физическое значение bias:
	В биологических нейронах это эквивалентно пороговому значению, которое необходимо достичь,\
	чтобы нейрон активировался. Нейрон не активируется, пока сумма входящих сигналов не превысит определенный порог.
	Bias позволяет нейрону "сдвинуть" функцию активации вверх или вниз, что помогает сети лучше моделировать сложные \
	зависимости между входами и выходами. Это делает нейрон более чувствительным к слабым или сильным сигналам.
    """


    def __init__(self, weights, bias):
        """

        :param weights: Веса (массив входных импульсов (весов)), которые нейрон использует для оценки важности
        каждого входного сигнала. Они соответствуют силе связи с каждым входом.
        :param bias:
        """

        self.weights = weights
        self.bias = bias  # Смещение

    def activation(self, x):
        """
        Физическое значение:
        Активационная функция в биологических нейронах моделирует процесс принятия решения нейроном: \
        активируется ли он (выдаст ли сигнал дальше), в зависимости от силы входного сигнала.

        Функция принимает значение x, которое может быть результатом взвешенной суммы входов.
        Затем она использует функцию ReLU (Rectified Linear Unit), которая возвращает максимум между нулем \
        и этим значением.
        Если значение x положительное, оно остается без изменений. Если x отрицательное, оно становится 0.

        :param x:
        :return:
        """

        # В данном примере используем ReLU в качестве активационной функции
        return np.maximum(0, x)

    def feedforward(self, inputs):   # прямая связь
        """

        :param inputs: Входной параметр, массив чисел (сигналы от других нейронов или сенсоров).

        :return: Финальный результат работы нейрона — это активированное значение, которое передается
         на следующий слой сети или используется в текущем процессе.

        Вычисления взвешенной суммы. Это аналог внутреннего произведения вектора входных данных и вектора весов.
        После вычисления взвешенной суммы, добавляется смещение (bias). Bias можно представить как базовый порог,
        который нейрон должен "перешагнуть", чтобы быть активированным.
        Это позволяет нейрону быть активным, даже если входные сигналы не очень сильные
        (например, входы могут быть равны нулю, но bias будет обеспечивать некое минимальное значение).

        Активация.
        После вычисления итоговой суммы (взвешенные входы + bias), результат передается в метод self.activation(total),
        который пропускает это значение через активационную функцию (в данном случае через ReLU).
        Активационная функция "решает", будет ли нейрон активирован, и какое значение он передаст дальше.
        """

        # Вычисляем взвешенную сумму входов и добавляем bias
        total = np.dot(self.weights, inputs) + self.bias
        # Пропускаем через активационную функцию
        return self.activation(total)



# # Пример использования нейрона
# weights = np.array([0.5, 0.3, 0.2])  # Веса нейрона
# bias = 0.7  # Смещение (bias)
# neuron = Neuron(weights, bias)
#
# inputs = np.array([1, 2, 3])  # Входные данные
# output = neuron.feedforward(inputs)  # Передача данных через нейрон
# print("Выход нейрона:", output)